{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6EDfWQLkBLX",
    "tags": []
   },
   "source": [
    "## 1. Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3NPCLzmkBLa"
   },
   "source": [
    "<center> <img src=https://storage.googleapis.com/kaggle-competitions/kaggle/3333/media/taxi_meter.png align=\"right\" width=\"300\"/> </center>\n",
    "    \n",
    "Вам предстоит решить настоящую задачу машинного обучения, направленную на автоматизацию бизнес процессов. Мы построим модель, которая будет предсказывать общую продолжительность поездки такси в Нью-Йорке. \n",
    "\n",
    "Представьте вы заказываете такси из одной точки Нью-Йорка в другую, причем не обязательно конечная точка должна находиться в пределах города. Сколько вы должны будете за нее заплатить? Известно, что стоимость такси в США  рассчитывается на основе фиксированной ставки + тарифная стоимость, величина которой зависит от времени и расстояния. Тарифы варьируются в зависимости от города.\n",
    "\n",
    "В свою очередь время поездки зависит от множества факторов таких как, откуда и куда вы едете, в какое время суток вы совершаете вашу поездку, погодных условий и так далее. \n",
    "\n",
    "Таким образом, если мы разработаем алгоритм, способный определять длительность поездки, мы сможем прогнозировать ее стоимость самым тривиальным образом, например, просто умножая стоимость на заданный тариф. \n",
    "Сервисы такси хранят огромные объёмы информации о поездках, включая такие данные как конечная, начальная точка маршрута, дата поездки и ее длительность. Эти данные можно использовать для того, чтобы прогнозировать длительность поездки в автоматическом режиме с привлечением искусственного интеллекта.\n",
    "\n",
    "**Бизнес-задача:** определить характеристики и с их помощью спрогнозировать длительность поездки такси.\n",
    "\n",
    "**Техническая задача для вас как для специалиста в Data Science:** построить модель машинного обучения, которая на основе предложенных характеристик клиента будет предсказывать числовой признак - время поездки такси. То есть решить задачу регрессии.\n",
    "\n",
    "**Основные цели проекта:**\n",
    "1. Сформировать набор данных на основе нескольких источников информации\n",
    "2. Спроектировать новые признаки с помощью Feature Engineering и выявить наиболее значимые при построении модели\n",
    "3. Исследовать предоставленные данные и выявить закономерности\n",
    "4. Построить несколько моделей и выбрать из них наилучшую по заданной метрике\n",
    "5. Спроектировать процесс предсказания времени длительности поездки для новых данных\n",
    "\n",
    "Загрузить свое решение на платформу Kaggle, тем самым поучаствовав в настоящем Data Science соревновании.\n",
    "Во время выполнения проекта вы отработаете навыки работы с несколькими источниками данных, генерации признаков, разведывательного анализа и визуализации данных, отбора признаков и, конечно же, построения моделей машинного обучения!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Up0IUABkBLc",
    "tags": []
   },
   "source": [
    "## 2. Знакомство с данными, базовый анализ и расширение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7W9mZPhWkBLd"
   },
   "source": [
    "Начнём наше исследование со знакомства с предоставленными данными. А также подгрузим дополнительные источники данных и расширим наш исходный датасет. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTIxJupXkBLd"
   },
   "source": [
    "Заранее импортируем модули, которые нам понадобятся для решения задачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yeC12P0hkBLe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import cluster\n",
    "from sklearn import feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kk5-kjqikBLf"
   },
   "source": [
    "Прочитаем наш файл с исходными данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "hMQnRJwdkBLg",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (1458644, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0  id2875421          2  2016-03-14 17:24:55  2016-03-14 17:32:30   \n",
       "1  id2377394          1  2016-06-12 00:43:35  2016-06-12 00:54:38   \n",
       "2  id3858529          2  2016-01-19 11:35:24  2016-01-19 12:10:48   \n",
       "3  id3504673          2  2016-04-06 19:32:31  2016-04-06 19:39:40   \n",
       "4  id2181028          2  2016-03-26 13:30:55  2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.765602                  N            455  \n",
       "1         40.731152                  N            663  \n",
       "2         40.710087                  N           2124  \n",
       "3         40.706718                  N            429  \n",
       "4         40.782520                  N            435  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data = pd.read_csv(\"C:\\\\Users\\\\ievge\\\\Desktop\\\\jupyter\\\\data\\\\train_new.csv\")\n",
    "print('Train data shape: {}'.format(taxi_data.shape))\n",
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhaxNt0XkBLh"
   },
   "source": [
    "Итак, у нас с вами есть данные о почти 1.5 миллионах поездок и 11 характеристиках, которые описывают каждую из поездок. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXufvADjkBLi"
   },
   "source": [
    "Мы условно разделили признаки нескольких групп. Каждой из групп мы в дальнейшем уделим отдельное внимание.\n",
    "\n",
    "**Данные о клиенте и таксопарке:**\n",
    "* id - уникальный идентификатор поездки\n",
    "* vendor_id - уникальный идентификатор поставщика (таксопарка), связанного с записью поездки\n",
    "\n",
    "**Временные характеристики:**\n",
    "* pickup_datetime - дата и время, когда был включен счетчик поездки\n",
    "* dropoff_datetime - дата и время, когда счетчик был отключен\n",
    "\n",
    "**Географическая информация:**\n",
    "* pickup_longitude -  долгота, на которой был включен счетчик\n",
    "* pickup_latitude - широта, на которой был включен счетчик\n",
    "* dropoff_longitude - долгота, на которой счетчик был отключен\n",
    "* dropoff_latitude - широта, на которой счетчик был отключен\n",
    "\n",
    "**Прочие признаки:**\n",
    "* passenger_count - количество пассажиров в транспортном средстве (введенное водителем значение)\n",
    "* store_and_fwd_flag - флаг, который указывает, сохранилась ли запись о поездке в памяти транспортного средства перед отправкой поставщику. Y - хранить и пересылать, N - не хранить и не пересылать поездку.\n",
    "\n",
    "**Целевой признак:**\n",
    "* trip_duration - продолжительность поездки в секундах\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvrcGynTkBLi"
   },
   "source": [
    "Для начала мы проведем базовый анализ того, насколько данные готовы к дальнейшей предобработке и анализу. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33rsP7rTkBLj"
   },
   "source": [
    "### Задание 2.1\n",
    "Для начала посмотрим на временные рамки, в которых мы работаем с данными.\n",
    "\n",
    "Переведите признак pickup_datetime в тип данных datetime с форматом год-месяц-день час:минута:секунда (в функции pd.to_datetime() параметр format='%Y-%m-%d %H:%M:%S'). \n",
    "\n",
    "Определите временные рамки (без учета времени), за которые представлены данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cIVxwqW1kBLj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 00:00:17\n",
      "2016-06-30 23:59:39\n"
     ]
    }
   ],
   "source": [
    "taxi_data['pickup_datetime'] = pd.to_datetime(taxi_data['pickup_datetime'])\n",
    "print(taxi_data['pickup_datetime'].min())\n",
    "print(taxi_data['pickup_datetime'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prw5RhKnkBLj"
   },
   "source": [
    "### Задание 2.2\n",
    "Посмотрим на пропуски. \n",
    "Сколько пропущенных значений присутствует в данных (суммарно по всем столбцам таблицы)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ug16JoZDkBLk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "vendor_id             0\n",
       "pickup_datetime       0\n",
       "dropoff_datetime      0\n",
       "passenger_count       0\n",
       "pickup_longitude      0\n",
       "pickup_latitude       0\n",
       "dropoff_longitude     0\n",
       "dropoff_latitude      0\n",
       "store_and_fwd_flag    0\n",
       "trip_duration         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMMIyT1skBLk"
   },
   "source": [
    "### Задание 2.3\n",
    "Посмотрим на статистические характеристики некоторых признаков. \n",
    "\n",
    "а) Сколько уникальных таксопарков присутствует в данных?\n",
    "\n",
    "б) Каково максимальное количество пассажиров?\n",
    "\n",
    "в) Чему равна средняя и медианная длительность поездки? Ответ приведите в секундах и округлите до целого.\n",
    "\n",
    "г) Чему равно минимальное и максимальное время поездки (в секундах)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UeJfdmIskBLk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique vendors: 2\n",
      "Maximun No of passengers: 9\n",
      "Mean and median trip duration: 959 s, 662 s\n",
      "Minimum and maximum trip duration: 1 s, 3526282 s\n"
     ]
    }
   ],
   "source": [
    "# Проверим сколько у нас таксопарков \n",
    "print(f\"Количество таксопарков: {taxi_data['vendor_id'].nunique()}\")\n",
    "\n",
    "# Проверим максимальное количество пасажиров \n",
    "print(f\"Максимальное количество пасажиров: {taxi_data['passenger_count'].max()}\")\n",
    "\n",
    "# Выведем среднее и медианую длительность поездок \n",
    "print(f\"Средняя продолжительность поездки: {round(taxi_data['trip_duration'].mean())}\")\n",
    "print(f\"Медианая продолжительность поездки: {round(taxi_data['trip_duration'].median())}\")\n",
    "\n",
    "# Минимальное и максимальное время поездки \n",
    "print(f\"Минимальное время поездки: {taxi_data['trip_duration'].min()}\")\n",
    "print(f\"Максимальное время поездки: {taxi_data['trip_duration'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhUgalQSkBLk"
   },
   "source": [
    "Займемся расширением исходного набора данных как с помощью внешних источников, так и с помощью манипуляций над имеющимися в данных признаками. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riVyyTGRkBLk"
   },
   "source": [
    "### Задание 2.4\n",
    "Реализуйте функцию add_datetime_features(), которая принимает на вход таблицу с данными о поездках (DataFrame) и возвращает ту же таблицу с добавленными в нее 3 столбцами:\n",
    "* pickup_date - дата включения счетчика - начала поездки (без времени);\n",
    "* pickup_hour - час дня включения счетчика;\n",
    "* pickup_day_of_week - наименование дня недели, в который был включен счетчик.\n",
    "\n",
    "а) Сколько поездок было совершено в субботу?\n",
    "\n",
    "б) Сколько поездок в среднем совершается в день? Ответ округлите до целого"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2987aTUAkBLl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>17</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "      <td>2016-06-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>11</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>19</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>13</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id     pickup_datetime     dropoff_datetime  \\\n",
       "0  id2875421          2 2016-03-14 17:24:55  2016-03-14 17:32:30   \n",
       "1  id2377394          1 2016-06-12 00:43:35  2016-06-12 00:54:38   \n",
       "2  id3858529          2 2016-01-19 11:35:24  2016-01-19 12:10:48   \n",
       "3  id3504673          2 2016-04-06 19:32:31  2016-04-06 19:39:40   \n",
       "4  id2181028          2 2016-03-26 13:30:55  2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration pickup_date  \\\n",
       "0         40.765602                  N            455  2016-03-14   \n",
       "1         40.731152                  N            663  2016-06-12   \n",
       "2         40.710087                  N           2124  2016-01-19   \n",
       "3         40.706718                  N            429  2016-04-06   \n",
       "4         40.782520                  N            435  2016-03-26   \n",
       "\n",
       "   pickup_hour pickup_day_of_week  \n",
       "0           17             Monday  \n",
       "1            0             Sunday  \n",
       "2           11            Tuesday  \n",
       "3           19          Wednesday  \n",
       "4           13           Saturday  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Напишем функцию \n",
    "def add_datetime_features(df):\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df['pickup_date'] = df['pickup_datetime'].dt.date\n",
    "    df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['pickup_day_of_week'] = df['pickup_datetime'].dt.day_of_week\n",
    "    return df\n",
    "\n",
    "# Применим нашу функцию\n",
    "taxi_data = add_datetime_features(taxi_data)\n",
    "\n",
    "mask = taxi_data['pickup_day_of_week'] == 5  # Saturday\n",
    "print('Rides on Saturday: {}'.format(\n",
    "    taxi_data[mask]['pickup_day_of_week'].count()))\n",
    "print('Average rides per day: {:.0f}'.format(\n",
    "    taxi_data.groupby('pickup_date')['id'].count().mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rCk6lyTkBLl"
   },
   "source": [
    "### Задание 2.5\n",
    "Реализуйте функцию add_holiday_features(), которая принимает на вход две таблицы: \n",
    "* таблицу с данными о поездках;\n",
    "* таблицу с данными о праздничных днях;\n",
    "\n",
    "и возвращает обновленную таблицу с данными о поездках с добавленным в нее столбцом pickup_holiday - бинарным признаком того, начата ли поездка в праздничный день или нет (1 - да, 0 - нет). \n",
    "\n",
    "Чему равна медианная длительность поездки на такси в праздничные дни? Ответ приведите в секундах, округлив до целого.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZnPWsSMukBLl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Напишем функцию для обновления выборки \n",
    "def add_holiday_features(df, holidays_df):\n",
    "    holidays = list(holidays_df['date'])\n",
    "    df['pickup_holiday'] = df['pickup_date'].apply(\n",
    "        lambda x: 1 if str(x) in holidays else 0)\n",
    "    return df\n",
    "\n",
    "# Загрузим даты праздничных дней \n",
    "holiday_data = pd.read_csv('C:\\\\Users\\\\ievge\\\\Desktop\\\\jupyter\\\\data\\\\holiday_data.csv', sep=';')\n",
    "# Применим нашу функцию \n",
    "taxi_data = add_holiday_features(taxi_data, holiday_data)\n",
    "\n",
    "# Проверим медианную длительность поездки в праздничные дни\n",
    "mask = taxi_data['pickup_holiday'] == 1  # Holidays\n",
    "print('Median trip duration on holidays: {:.0f} s'.format(\n",
    "    taxi_data[mask]['trip_duration'].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPxMlVMAkBLl"
   },
   "source": [
    "### Задание 2.6\n",
    "Реализуйте функцию add_osrm_features(), которая принимает на вход две таблицы:\n",
    "* таблицу с данными о поездках;\n",
    "* таблицу с данными из OSRM;\n",
    "\n",
    "и возвращает обновленную таблицу с данными о поездках с добавленными в нее 3 столбцами:\n",
    "* total_distance;\n",
    "* total_travel_time;\n",
    "* number_of_steps.\n",
    "\n",
    "а) Чему равна разница (в секундах) между медианной длительностью поездки в данных и медианной длительностью поездки, полученной из OSRM? \n",
    "\n",
    "В результате объединения таблиц у вас должны были получиться пропуски в столбцах с информацией из OSRM API. Это связано с тем, что для некоторых поездок не удалось выгрузить данные из веб источника. \n",
    "\n",
    "б) Сколько пропусков содержится в столбцах с информацией из OSRM API после объединения таблиц?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uey_zFbwkBLm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372.5\n"
     ]
    }
   ],
   "source": [
    "# Напишем функцию для объединения таблиц \n",
    "def add_osrm_features(df, osrm_df):\n",
    "    osrm = osrm_df[[\n",
    "        'id', 'total_distance', 'total_travel_time', 'number_of_steps']]\n",
    "    df = pd.merge(df, osrm, how='left', on='id')\n",
    "    return df\n",
    "\n",
    "\n",
    "osrm_data = pd.read_csv('C:\\\\Users\\\\ievge\\\\Desktop\\\\jupyter\\\\data\\\\osrm_data_train.csv')\n",
    "# Применим функцию\n",
    "taxi_data = add_osrm_features(taxi_data, osrm_data)\n",
    "# Проверим медианную длинну поездки \n",
    "diff = taxi_data['trip_duration'].median() \\\n",
    "    - taxi_data['total_travel_time'].median()\n",
    "print('Duration difference: {:.2f} s'.format(diff))\n",
    "\n",
    "\n",
    "osrm_cols = ['total_distance', 'total_travel_time', 'number_of_steps']\n",
    "display(taxi_data[osrm_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ti1rljMGkBLm"
   },
   "outputs": [],
   "source": [
    "def get_haversine_distance(lat1, lng1, lat2, lng2):\n",
    "    # переводим углы в радианы\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    # радиус земли в километрах\n",
    "    EARTH_RADIUS = 6371 \n",
    "    # считаем кратчайшее расстояние h по формуле Хаверсина\n",
    "    lat_delta = lat2 - lat1\n",
    "    lng_delta = lng2 - lng1\n",
    "    d = np.sin(lat_delta*0.5)**2 \\\n",
    "        + np.cos(lat1)*np.cos(lat2)*np.sin(lng_delta*0.5)**2\n",
    "    h = 2 * EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "\n",
    "def get_angle_direction(lat1, lng1, lat2, lng2):\n",
    "    # переводим углы в радианы\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    # считаем угол направления движения alpha по формуле угла пеленга\n",
    "    lng_delta_rad = lng2 - lng1\n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n",
    "    x = np.cos(lat1)*np.sin(lat2) \\\n",
    "        - np.sin(lat1)*np.cos(lat2)*np.cos(lng_delta_rad)\n",
    "    alpha = np.degrees(np.arctan2(y, x))\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGQpi4erkBLm",
    "tags": []
   },
   "source": [
    "### Задание 2.7.\n",
    "Реализуйте функцию add_geographical_features(), которая принимает на вход таблицу с данными о поездках и возвращает обновленную таблицу с добавленными в нее 2 столбцами:\n",
    "* haversine_distance - расстояние Хаверсина между точкой, в которой был включен счетчик, и точкой, в которой счетчик был выключен;\n",
    "* direction - направление движения из точки, в которой был включен счетчик, в точку, в которой счетчик был выключен.\n",
    "\n",
    "Чему равно медианное расстояние Хаверсина поездок (в киллометрах)? Ответ округлите до сотых.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zqIZyeHmkBLm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.09"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Напишем функцию \n",
    "def add_geographical_features(df):\n",
    "    lat1, lng1 = df['pickup_latitude'], df['pickup_longitude']\n",
    "    lat2, lng2 = df['dropoff_latitude'], df['dropoff_longitude']\n",
    "    df['haversine_distance'] = get_haversine_distance(lat1, lng1, lat2, lng2)\n",
    "    df['direction'] = get_angle_direction(lat1, lng1, lat2, lng2)\n",
    "    return df\n",
    "\n",
    "# Применим нашу функцию\n",
    "taxi_data = add_geographical_features(taxi_data)\n",
    "\n",
    "# Проверим медианное расстояние Хаверсина\n",
    "print('Median haversine distance: {:.2f} km'.format(\n",
    "    taxi_data['haversine_distance'].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WWydgB2kBLm"
   },
   "source": [
    "### Задание 2.8.\n",
    "Реализуйте функцию add_cluster_features(), которая принимает на вход таблицу с данными о поездках и обученный алгоритм кластеризации. Функция должна возвращать обновленную таблицу с добавленными в нее столбцом geo_cluster - географический кластер, к которому относится поездка.\n",
    "\n",
    "Сколько поездок содержится в наименьшем по размеру географическом кластере?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YxpWNYfakBLn"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10648\\2999225854.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtaxi_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_cluster_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaxi_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mtaxi_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'geo_cluster'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10648\\2999225854.py\u001b[0m in \u001b[0;36madd_cluster_features\u001b[1;34m(df, model)\u001b[0m\n\u001b[0;32m     11\u001b[0m     coords = np.hstack((df[['pickup_latitude', 'pickup_longitude']],\n\u001b[0;32m     12\u001b[0m                         df[['dropoff_latitude', 'dropoff_longitude']]))\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'geo_cluster'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, sample_weight)\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m         return _labels_inertia_threadpool_limit(\n\u001b[0m\u001b[0;32m   1335\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_threads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m         )[0]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36m_labels_inertia_threadpool_limit\u001b[1;34m(X, sample_weight, x_squared_norms, centers, n_threads)\u001b[0m\n\u001b[0;32m    753\u001b[0m ):\n\u001b[0;32m    754\u001b[0m     \u001b[1;34m\"\"\"Same as _labels_inertia but in a threadpool_limits context.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mthreadpool_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"blas\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         labels, inertia = _labels_inertia(\n\u001b[0;32m    757\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36mthreadpool_limits\u001b[1;34m(limits, user_api)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mthreadpoolctl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreadpool_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, limits, user_api)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_threadpool_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m_set_threadpool_limits\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         modules = _ThreadpoolInfo(prefixes=self._prefixes,\n\u001b[0m\u001b[0;32m    269\u001b[0m                                   user_api=self._user_api)\n\u001b[0;32m    270\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_if_incompatible_openmp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m_load_modules\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_modules_with_dyld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"win32\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_modules_with_enum_process_module_ex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_modules_with_dl_iterate_phdr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m_find_modules_with_enum_process_module_ex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m                 \u001b[1;31m# Store the module if it is supported and selected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_module_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mkernel_32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCloseHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_process\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m_make_module_from_path\u001b[1;34m(self, filepath)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefixes\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0muser_api\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[0mmodule_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule_class\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m                 \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath, prefix, user_api, internal_api)\u001b[0m\n\u001b[0;32m    604\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_api\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minternal_api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynlib\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_RTLD_NOLOAD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_threads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_threads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_extra_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py\u001b[0m in \u001b[0;36mget_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    644\u001b[0m                              lambda: None)\n\u001b[0;32m    645\u001b[0m         \u001b[0mget_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb\"OpenBLAS\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# Создаем обучающую выборку из географических координат всех точек\n",
    "def add_cluster_features(df, cluster_model):\n",
    "    X = np.array(df[['pickup_latitude', 'pickup_longitude',\n",
    "                     'dropoff_latitude', 'dropoff_longitude']])\n",
    "    df['geo_cluster'] = cluster_model.predict(X)\n",
    "    return df\n",
    "  \n",
    "\n",
    "coords = np.hstack((taxi_data[['pickup_latitude', 'pickup_longitude']],\n",
    "                    taxi_data[['dropoff_latitude', 'dropoff_longitude']]))\n",
    "# обучаем алгоритм кластеризации\n",
    "kmeans = cluster.KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(coords)\n",
    "\n",
    "# Применяем нашу функцию\n",
    "taxi_data = add_cluster_features(taxi_data, kmeans)\n",
    "\n",
    "# Проверяем количество поездок по кластерам\n",
    "cluster_pivot = taxi_data.groupby('geo_cluster')['id'].count()\n",
    "print('Rides in the smallest cluster: {}'.format(cluster_pivot.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FRtxWNDkBLn"
   },
   "source": [
    "### Задание 2.9.\n",
    "Реализуйте функцию add_weather_features(), которая принимает на вход две таблицы:\n",
    "* таблицу с данными о поездках;\n",
    "* таблицу с данными о погодных условиях на каждый час;\n",
    "\n",
    "и возвращает обновленную таблицу с данными о поездках с добавленными в нее 5 столбцами:\n",
    "* temperature - температура;\n",
    "* visibility - видимость;\n",
    "* wind speed - средняя скорость ветра;\n",
    "* precip - количество осадков;\n",
    "* events - погодные явления.\n",
    "\n",
    "а) Сколько поездок было совершено в снежную погоду?\n",
    "\n",
    "В результате объединения у вас должны получиться записи, для которых в столбцах temperature, visibility, wind speed, precip, и events будут пропуски. Это связано с тем, что в таблице с данными о погодных условиях отсутствуют измерения для некоторых моментов времени, в которых включался счетчик поездки. \n",
    "\n",
    "б) Сколько процентов от общего количества наблюдений в таблице с данными о поездках занимают пропуски в столбцах с погодными условиями? Ответ приведите с точностью до сотых процента.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdgHJwQgkBLn"
   },
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv('C:\\\\Users\\\\ievge\\\\Desktop\\\\jupyter\\\\data\\\\weather_data.csv')\n",
    "\n",
    "# Напишем функцию\n",
    "def add_weather_features(df, weather_df):\n",
    "    weather_df['time'] = pd.to_datetime(weather_df['time'])\n",
    "    weather_df['date'] = weather_df['time'].dt.date\n",
    "    weather_df['hour'] = weather_df['time'].dt.hour\n",
    "    cols = ['date', 'hour', \n",
    "            'temperature', 'visibility', 'wind speed', 'precip', 'events']\n",
    "    df = pd.merge(df, weather_df[cols], how='left',\n",
    "        left_on=['pickup_date', 'pickup_hour'], right_on=['date', 'hour'])\n",
    "    df.drop(columns=['date', 'hour'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Применяем нашу функцию\n",
    "taxi_data = add_weather_features(taxi_data, weather_data)\n",
    "\n",
    "# Проверим сколько поездок было зимой\n",
    "mask = taxi_data['events'] == 'Snow'\n",
    "print('Rides in snow: {}'.format(taxi_data[mask].shape[0]))\n",
    "\n",
    "# Percent of missing values:\n",
    "weather_cols = ['temperature', 'visibility', 'wind speed', 'precip', 'events']\n",
    "display(taxi_data[weather_cols].isnull().mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwsluTQHkBLn"
   },
   "source": [
    "### Задание 2.10.\n",
    "Реализуйте функцию fill_null_weather_data(), которая принимает на вход которая принимает на вход таблицу с данными о поездках. Функция должна заполнять пропущенные значения в столбцах.\n",
    "\n",
    "Пропуски в столбцах с погодными условиями -  temperature, visibility, wind speed, precip заполните медианным значением температуры, влажности, скорости ветра и видимости в зависимости от даты начала поездки. Для этого сгруппируйте данные по столбцу pickup_date и рассчитайте медиану в каждой группе, после чего с помощью комбинации методов transform() и fillna() заполните пропуски. \n",
    "Пропуски в столбце events заполните строкой 'None' - символом отсутствия погодных явлений (снега/дождя/тумана). \n",
    "\n",
    "Пропуски в столбцах с информацией из OSRM API - total_distance, total_travel_time и number_of_steps заполните медианным значением по столбцам. \n",
    "\n",
    "Чему равна медиана в столбце temperature после заполнения пропусков? Ответ округлите до десятых.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHIYqxjKkBLn"
   },
   "outputs": [],
   "source": [
    "# Напишем функцию\n",
    "def fill_null_weather_data(df):\n",
    "    df['temperature'] = df['temperature'].fillna(\n",
    "        df.groupby('pickup_date')['temperature'].transform('median'))\n",
    "    df['visibility'] = df['visibility'].fillna(\n",
    "        df.groupby('pickup_date')['visibility'].transform('median'))\n",
    "    df['wind speed'] = df['wind speed'].fillna(\n",
    "        df.groupby('pickup_date')['wind speed'].transform('median'))\n",
    "    df['precip'] = df['precip'].fillna(\n",
    "        df.groupby('pickup_date')['precip'].transform('median'))\n",
    "    \n",
    "    df['events'] = df['events'].fillna('None')\n",
    "    \n",
    "    df['total_distance'] = df['total_distance'].fillna(\n",
    "        df['total_distance'].median())\n",
    "    df['total_travel_time'] = df['total_travel_time'].fillna(\n",
    "        df['total_travel_time'].median())\n",
    "    df['number_of_steps'] = df['number_of_steps'].fillna(\n",
    "        df['number_of_steps'].median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Применяем нашу функцию\n",
    "taxi_data = fill_null_weather_data(taxi_data)\n",
    "\n",
    "# Проверим медиану признака temperature\n",
    "print('Median temperature: {:.1f}'.format(taxi_data['temperature'].median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MahzwVDxkBLn"
   },
   "source": [
    "В завершение первой части найдем очевидные выбросы в целевой переменной - длительности поездки. \n",
    "\n",
    "Проще всего найти слишком продолжительные поездки. Давайте условимся, что выбросами будут считаться поездки, длительность которых превышает 24 часа. \n",
    "\n",
    "Чуть сложнее с анализом поездок, длительность которых слишком мала. Потому что к ним относятся действительно реальные поездки на короткие расстояния, поездки, которые были отменены через секунду после того как включился счетчик, а также “телепортации” - перемещение на большие расстояния за считанные секунды. \n",
    "Условимся, что мы будем считать выбросами только последнюю группу. Как же нам их обнаружить наиболее простым способом?\n",
    "\n",
    "Можно воспользоваться информацией о кратчайшем расстоянии, которое проезжает такси. Вычислить среднюю скорость автомобиля на кратчайшем пути следующим образом: \n",
    "$$avg\\_speed= \\frac{total\\_distance}{1000*trip\\_duration}*3600$$\n",
    "Если мы построим диаграмму рассеяния средней скорости движения автомобилей, мы увидим следующую картину:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nnp1KatIkBLo"
   },
   "outputs": [],
   "source": [
    "avg_speed = taxi_data['total_distance'] / taxi_data['trip_duration'] * 3.6\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.scatterplot(x=avg_speed.index, y=avg_speed, ax=ax)\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Average speed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGxXxv7LkBLo"
   },
   "source": [
    "Как раз отсюда мы видим, что у нас есть “поездки-телепортации”, для которых средняя скорость более 1000 км/ч. Даже есть такая, средняя скорость которой составляла более 12000 км/ч! \n",
    "\n",
    "Давайте условимся, что предельная средняя скорость, которую могут развивать таксисты будет 300 км/ч. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSWHdHNWkBLo",
    "tags": []
   },
   "source": [
    "### Задание 2.11.\n",
    "Найдите поездки, длительность которых превышает 24 часа. И удалите их из набора данных.\n",
    "\n",
    "а) Сколько выбросов по признаку длительности поездки вам удалось найти?\n",
    "\n",
    "Найдите поездки, средняя скорость которых по кратчайшему пути превышает 300 км/ч и удалите их из данных. \n",
    "\n",
    "б) Сколько выбросов по признаку скорости вам удалось найти?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KVmlq8SkBLo"
   },
   "outputs": [],
   "source": [
    "# Проверим признак длительности поездки\n",
    "mask = taxi_data['trip_duration'] > 86400\n",
    "print('Number of trips > 24 h: {}'.format(taxi_data[mask].shape[0]))\n",
    "taxi_data = taxi_data[~mask]\n",
    "\n",
    "mask = (taxi_data['total_distance'] / taxi_data['trip_duration'] * 3.6) > 300\n",
    "print(\"Number of 'teleportations': {}\".format(taxi_data[mask].shape[0]))\n",
    "taxi_data = taxi_data[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEEXbpTfkBLo",
    "tags": []
   },
   "source": [
    "## 3. Разведывательный анализ данных (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59de6wa-kBLo"
   },
   "source": [
    "В этой части нашего проекта мы с вами:\n",
    "* Исследуем сформированный набор данных; \n",
    "* Попробуем найти закономерности, позволяющие сформулировать предварительные гипотезы относительно того, какие факторы являются решающими в определении длительности поездки;\n",
    "* Дополним наш анализ визуализациями, иллюстрирующими; исследование. Постарайтесь оформлять диаграммы с душой, а не «для галочки»: навыки визуализации полученных выводов обязательно пригодятся вам в будущем.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-8PQtqRkBLo"
   },
   "source": [
    "Начинаем с целевого признака. Забегая вперед, скажем, что основной метрикой качества решения поставленной задачи будет RMSLE - Root Mean Squared Log Error, которая вычисляется на основе целевой переменной в логарифмическом масштабе. В таком случае целесообразно сразу логарифмировать признак длительности поездки и рассматривать при анализе логарифм в качестве целевого признака:\n",
    "$$trip\\_duration\\_log = log(trip\\_duration+1),$$\n",
    "где под символом log подразумевается натуральный логарифм.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y59BwN3MkBLp"
   },
   "outputs": [],
   "source": [
    "taxi_data['trip_duration_log'] = np.log(taxi_data['trip_duration']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoqAWXLLkBLp"
   },
   "source": [
    "### Задание 3.1.\n",
    "Постройте гистограмму и коробчатую диаграмму длительности поездок в логарифмическом масштабе (trip_duration_log). \n",
    "Исходя из визуализации, сделайте предположение, является ли полученное распределение нормальным? \n",
    "Проверьте свою гипотезу с помощью теста Д’Агостино при уровне значимости $\\alpha=0.05$. \n",
    "\n",
    "а) Чему равен вычисленный p-value? Ответ округлите до сотых.\n",
    "\n",
    "б) Является ли распределение длительности поездок в логарифмическом масштабе нормальным?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cea6d1f4372850e6c5692d7be7ca1cee3bdb42e",
    "id": "hSPVCtNckBLp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(taxi_data,\n",
    "                  x='trip_duration_log',\n",
    "                  title='Trip duration distribution',\n",
    "                  marginal='box',\n",
    "                  nbins=150,\n",
    "                  width=650,\n",
    "                  height=400                  \n",
    ")\n",
    "fig.layout.xaxis.title.text = 'Logarithm of trip duration, [s]'\n",
    "fig.layout.yaxis.title.text = 'Number of trips'\n",
    "fig.show('svg')\n",
    "\n",
    "# The distribution is not normal (p-value = 0): \n",
    "_, p = stats.normaltest(taxi_data['trip_duration_log'])\n",
    "print('p-value = {:.2f}'.format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teZmcjlIkBLp"
   },
   "source": [
    "### Задание 3.2.\n",
    "Постройте визуализацию, которая позволит сравнить распределение длительности поездки в логарифмическом масштабе (trip_duration_log) в зависимости от таксопарка (vendor_id). \n",
    "\n",
    "Сравните два распределения между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfyhOuOYkBLp"
   },
   "outputs": [],
   "source": [
    "# Зафиксируем таксопарки\n",
    "fig = px.box(taxi_data, \n",
    "             x='trip_duration_log',\n",
    "             color='vendor_id',\n",
    "             title='Trip duration by vendors',\n",
    "             width=650,\n",
    "             height=400\n",
    ")\n",
    "fig.layout.legend.title = 'Vendors'\n",
    "fig.layout.xaxis.title.text = 'Logarithm of trip duration, [s]'\n",
    "fig.show('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq7d2zjjkBLp"
   },
   "source": [
    "### Задание 3.3.\n",
    "Постройте визуализацию, которая позволит сравнить распределение длительности поездки в логарифмическом масштабе (trip_duration_log) в зависимости от признака отправки сообщения поставщику (store_and_fwd_flag). \n",
    "\n",
    "Сравните два распределения между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3If4bSdEkBLp"
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(taxi_data,\n",
    "                  x='trip_duration_log',\n",
    "                  title='Trip duration distribution',\n",
    "                  color='store_and_fwd_flag',\n",
    "                  histnorm='percent',\n",
    "                  barmode='group',\n",
    "                  marginal='box',\n",
    "                  nbins=100,\n",
    "                  width=650,\n",
    "                  height=400                  \n",
    ")\n",
    "fig.layout.xaxis.title.text = 'Logarithm of trip duration, [s]'\n",
    "fig.layout.yaxis.title.text = 'Number of trips, %'\n",
    "fig.layout.legend.title = 'Message'\n",
    "fig.show('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThgfBISekBLp"
   },
   "source": [
    "### Задание 3.4.\n",
    "Постройте две визуализации:\n",
    "* Распределение количества поездок в зависимости от часа дня;\n",
    "* Зависимость медианной длительности поездки от часа дня.\n",
    "\n",
    "На основе построенных графиков ответьте на следующие вопросы:\n",
    "\n",
    "а) В какое время суток такси заказывают реже всего?\n",
    "\n",
    "б) В какое время суток наблюдается пик медианной длительности поездок?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7tRWY_BkBLq"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n",
    "sns.countplot(data=taxi_data, x='pickup_hour', ax=axes[0])\n",
    "sns.barplot(data=taxi_data, x='pickup_hour', y='trip_duration', \n",
    "            estimator=np.median, ax=axes[1])\n",
    "axes[0].set_title('Trips per hour')\n",
    "axes[0].set_xlabel('Hour')\n",
    "axes[0].set_ylabel('Number of trips')\n",
    "axes[1].set_title('Median trip duration per hour')\n",
    "axes[1].set_xlabel('Hour')\n",
    "axes[1].set_ylabel('Median trip duration, s')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmHyGkbzkBLq"
   },
   "source": [
    "### Задание 3.5.\n",
    "Постройте две визуализации:\n",
    "* Распределение количества поездок в зависимости от дня недели;\n",
    "*  Зависимость медианной длительности поездки от дня недели.\n",
    "\n",
    "На основе построенных графиков ответьте на следующие вопросы:\n",
    "а) В какой день недели совершается больше всего поездок?\n",
    "б) В какой день недели медианная длительность поездок наименьшая?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0mHfS17kBLq"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n",
    "sns.countplot(data=taxi_data, x='pickup_day_of_week', ax=axes[0])\n",
    "sns.barplot(data=taxi_data, x='pickup_day_of_week', y='trip_duration',\n",
    "            estimator=np.median, ax=axes[1])\n",
    "\n",
    "xticks = ['Mon', 'Tue', 'Wen', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[0].set_title('Trips in the week')\n",
    "axes[0].set_xlabel('Day of week')\n",
    "axes[0].set_ylabel('Number of trips')\n",
    "axes[0].set_xticks(ticks=range(7), labels=xticks)\n",
    "\n",
    "axes[1].set_title('Median trip duration in the week')\n",
    "axes[1].set_xlabel('Day of week')\n",
    "axes[1].set_ylabel('Median trip duration, s')\n",
    "axes[1].set_xticks(ticks=range(7), labels=xticks)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSRSS4tekBLq"
   },
   "source": [
    "### Задание 3.6.\n",
    "Посмотрим на обе временные характеристики одновременно. \n",
    "\n",
    "Постройте сводную таблицу, по строкам которой отложены часы (pickup_hour), по столбцам - дни недели (pickup_day_of_week), а в ячейках - медианная длительность поездки (trip_duration). \n",
    "\n",
    "Визуализируйте полученную сводную таблицу с помощью тепловой карты (рекомендуемая палитра - coolwarm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDUyBJTQkBLq"
   },
   "outputs": [],
   "source": [
    "trip_duration_pivot = pd.pivot_table(\n",
    "    taxi_data, values='trip_duration', index='pickup_hour', \n",
    "    columns='pickup_day_of_week', aggfunc='median')\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 5))\n",
    "xticks = ['Mon', 'Tue', 'Wen', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "sns.heatmap(trip_duration_pivot, cmap='coolwarm', \n",
    "            xticklabels=xticks, yticklabels=2)\n",
    "axes.set_title('Median trip duration heatmap')\n",
    "axes.set_xlabel('Day of week')\n",
    "axes.set_ylabel('Hour');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5P2WBT6kBLq",
    "tags": []
   },
   "source": [
    "### Задание 3.7.\n",
    "Постройте две диаграммы рассеяния (scatter-диаграммы):\n",
    "* первая должна иллюстрировать географическое расположение точек начала поездок (pickup_longitude, pickup_latitude) \n",
    "* вторая должна географическое расположение точек завершения поездок (dropoff_longitude, dropoff_latitude).\n",
    "\n",
    "Для этого на диаграммах по оси абсцисс отложите широту (longitude), а по оси ординат - долготу (latitude). \n",
    "Включите в визуализацию только те точки, которые находятся в пределах Нью-Йорка - добавьте следующие ограничения на границы осей абсцисс и ординат:\n",
    " \n",
    "city_long_border = (-74.03, -73.75)\n",
    "\n",
    "city_lat_border = (40.63, 40.85)\n",
    "\n",
    "Добавьте на диаграммы расцветку по десяти географическим кластерам (geo_cluster), которые мы сгенерировали ранее. \n",
    "\n",
    "**Рекомендация:** для наглядности уменьшите размер точек на диаграмме рассеяния.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0Wd1q_PkBLq"
   },
   "outputs": [],
   "source": [
    "city_long_border = (-74.03, -73.75)\n",
    "city_lat_border = (40.63, 40.85)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n",
    "sns.scatterplot(data=taxi_data, x='pickup_longitude', y='pickup_latitude', \n",
    "                hue='geo_cluster', palette='tab10', s=10, ax=axes[0])\n",
    "sns.scatterplot(data=taxi_data, x='dropoff_longitude', y='dropoff_latitude', \n",
    "                hue='geo_cluster', palette='tab10', s=10, ax=axes[1])\n",
    "\n",
    "axes[0].legend(loc='upper right', title='Cluster')\n",
    "axes[0].set_xlim(city_long_border)\n",
    "axes[0].set_ylim(city_lat_border)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].set_title('Depature points')\n",
    "axes[0].set_xlabel('Longitude')\n",
    "axes[0].set_ylabel('Latitude')\n",
    "\n",
    "axes[1].legend(loc='upper right', title='Cluster')\n",
    "axes[1].set_xlim(city_long_border)\n",
    "axes[1].set_ylim(city_lat_border)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].set_title('Arrival points')\n",
    "axes[1].set_xlabel('Longitude')\n",
    "axes[1].set_ylabel('Latitude')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSKZEkO4kBLq"
   },
   "source": [
    "## 4. Отбор и преобразование признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erg_Ejz1kBLr"
   },
   "source": [
    "Перед тем как перейти к построению модели, осталось сделать ещё несколько шагов.\n",
    "* Следует помнить, что многие алгоритмы машинного обучения не могут обрабатывать категориальные признаки в их обычном виде. Поэтому нам необходимо их закодировать;\n",
    "* Надо отобрать признаки, которые мы будем использовать для обучения модели;\n",
    "*  Необходимо масштабировать и трансформировать некоторые признаки для того, чтобы улучшить сходимость моделей, в основе которых лежат численные методы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7MHv9HKkBLr"
   },
   "outputs": [],
   "source": [
    "print('Shape of data: {}'.format(taxi_data.shape))\n",
    "print('Columns: {}'.format(taxi_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6sgAOFqkBLr"
   },
   "source": [
    "Для удобства работы сделаем копию исходной таблицы с поездками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rxobyQFkBLr"
   },
   "outputs": [],
   "source": [
    "train_data = taxi_data.copy()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7i7DFBdkBLr"
   },
   "source": [
    "### Задание 4.1.\n",
    "Сразу позаботимся об очевидных неинформативных и избыточных признаках. \n",
    "\n",
    "а) Какой из признаков является уникальным для каждой поездки и не несет полезной информации в определении ее продолжительности?\n",
    "\n",
    "б) Утечка данных (data leak) - это…\n",
    "\n",
    "в) Подумайте, наличие какого из признаков в обучающем наборе данных создает утечку данных?\n",
    "\n",
    "г) Исключите выбранные в пунктах а) и в) признаки из исходной таблицы с данными. Сколько столбцов в таблице у вас осталось?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hg53Bv1kBLr"
   },
   "outputs": [],
   "source": [
    "train_data.drop(columns=['id', 'dropoff_datetime'], inplace=True)\n",
    "print('Number of columns: {}'.format(train_data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHyYd7XzkBLr"
   },
   "source": [
    "Ранее мы извлекли всю необходимую для нас информацию из даты начала поездки, теперь мы можем избавиться от этих признаков, так как они нам больше не понадобятся:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzO9ioMHkBLs"
   },
   "outputs": [],
   "source": [
    "drop_columns = ['pickup_datetime', 'pickup_date']\n",
    "train_data = train_data.drop(drop_columns, axis=1)\n",
    "print('Shape of data:  {}'.format(train_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXHbkc3ykBLs"
   },
   "source": [
    "### Задание 4.2.\n",
    "\n",
    "Закодируйте признак vendor_id в таблице train_data таким образом, чтобы он был равен 0, если идентификатор таксопарка равен 1, и 1 — в противном случае.\n",
    "\n",
    "Закодируйте признак store_and_fwd_flag в таблице train_data таким образом, чтобы он был равен 0, если флаг выставлен в значение 'N', и 1 — в противном случае.\n",
    "\n",
    "а) Рассчитайте среднее по закодированному столбцу vendor_id. Ответ приведите с точностью до сотых.\n",
    "\n",
    "б) Рассчитайте среднее по закодированному столбцу store_and_fwd_flag. Ответ приведите с точностью до тысячных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_O1jJaZtkBLs"
   },
   "outputs": [],
   "source": [
    "train_data['vendor_id'] = train_data['vendor_id'].replace({1: 0, 2: 1})\n",
    "train_data['store_and_fwd_flag'] = train_data['store_and_fwd_flag'].replace(\n",
    "    {'N': 0, 'Y': 1})\n",
    "\n",
    "print('Mean vendor id: {:.2f}'.format(train_data['vendor_id'].mean()))\n",
    "print(\"Mean 'store_and_fwd_flag': {:.3f}\".format(\n",
    "    train_data['store_and_fwd_flag'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMUmtLu1kBLs"
   },
   "source": [
    "### Задание 4.3.\n",
    "Создайте таблицу data_onehot из закодированных однократным кодированием признаков pickup_day_of_week, geo_cluster и events в таблице train_data с помощью OneHotEncoder из библиотеки sklearn. Параметр drop выставите в значение 'first', чтобы удалять первый бинарный столбец, тем самым не создавая излишних признаков.\n",
    "\n",
    "В результате работы OneHotEncoder вы получите безымянный numpy-массив, который нам будет необходимо преобразовать обратно в DataFrame, для более удобной работы в дальнейшем. Чтобы получить имена закодированных столбцов у объекта типа OneHotEncoder есть специальный метод get_feature_names_out(). Он возвращает список новых закодированных имен столбцов в формате <оригинальное имя столбца>_<имя категории>.\n",
    "\n",
    "Пример использования:\n",
    "\n",
    "``` python\n",
    "# Получаем закодированные имена столбцов\n",
    "column_names = one_hot_encoder.get_feature_names_out()\n",
    "# Составляем DataFrame из закодированных признаков\n",
    "data_onehot = pd.DataFrame(data_onehot, columns=column_names)\n",
    "```\n",
    "\n",
    "В этом псевдокоде:\n",
    "* one_hot_encoder - объект класса OneHotEncoder\n",
    "* data_onehot - numpy-массив, полученный в результате трансформации кодировщиком\n",
    "\n",
    "В результате выполнения задания у вас должен быть образован DataFrame `data_onehot`, который содержит кодированные категориальные признаки pickup_day_of_week, geo_cluster и events. \n",
    "\n",
    "\n",
    "Сколько бинарных столбцов у вас получилось сгенерировать с помощью однократного кодирования?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BonLFJbhkBLs"
   },
   "outputs": [],
   "source": [
    "encoder = preprocessing.OneHotEncoder(drop='first', sparse=False)\n",
    "columns_to_change = ['pickup_day_of_week', 'geo_cluster', 'events']\n",
    "encoder.fit(train_data[columns_to_change])\n",
    "data_onehot = pd.DataFrame(\n",
    "    encoder.transform(train_data[columns_to_change]),\n",
    "    columns=encoder.get_feature_names_out(columns_to_change))\n",
    "\n",
    "print('Number of encoded columns: {}'.format(data_onehot.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsGsYqHRkBLs"
   },
   "source": [
    "Добавим полученную таблицу с закодированными признаками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blXeWiS0kBLs"
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat(\n",
    "    [train_data.reset_index(drop=True).drop(columns_to_change, axis=1), data_onehot], \n",
    "    axis=1\n",
    ")\n",
    "print('Shape of data: {}'.format(train_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQhXAud5kBLt"
   },
   "source": [
    "Теперь, когда категориальные признаки предобработаны, сформируем матрицу наблюдений X, вектор целевой переменной y и его логарифм y_log. В матрицу наблюдений войдут все столбцы из таблицы с поездками за исключением целевого признака trip_duration и его логарифмированной версии trip_duration_log:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vz6YfcALkBLt"
   },
   "outputs": [],
   "source": [
    "X = train_data.drop(['trip_duration', 'trip_duration_log'], axis=1)\n",
    "y = train_data['trip_duration']\n",
    "y_log = train_data['trip_duration_log']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orPjdejMkBLt"
   },
   "source": [
    "Все наши модели мы будем обучать на логарифмированной версии y_log. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8CzG9CnkBLt"
   },
   "source": [
    "Выбранный тип валидации - hold-out. Разобьем выборку на обучающую и валидационную в соотношении 67/33:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPEJ89ZdkBLt"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train_log, y_valid_log = model_selection.train_test_split(\n",
    "    X, y_log, \n",
    "    test_size=0.33, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yL9ZPlW3kBLt"
   },
   "source": [
    "На данный момент у нас достаточно много признаков: скорее всего, не все из них будут важны. Давайте оставим лишь те, которые сильнее всего связаны с целевой переменной и точно будут вносить вклад в повышение качества модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaqdJUp-kBLt"
   },
   "source": [
    "### Задание 4.4.\n",
    "С помощью SelectKBest отберите 25 признаков, наилучшим образом подходящих для предсказания целевой переменной в логарифмическом масштабе. Отбор реализуйте по обучающей выборке, используя параметр score_func = f_regression.\n",
    "\n",
    "Укажите признаки, которые вошли в список отобранных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEeqoG20kBLt"
   },
   "outputs": [],
   "source": [
    "selector = feature_selection.SelectKBest(\n",
    "    score_func=feature_selection.f_regression, k=25)\n",
    "\n",
    "selector.fit(X_train, y_train_log)\n",
    "X_train = selector.transform(X_train)\n",
    "X_valid = selector.transform(X_valid)\n",
    "\n",
    "feature_names = train_data.columns.drop(['trip_duration', 'trip_duration_log'])\n",
    "selected_features = selector.get_feature_names_out(feature_names)\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLOJ_kdlkBLt"
   },
   "source": [
    "Так как мы будем использовать различные модели, в том числе внутри которых заложены численные методы оптимизации, то давайте заранее позаботимся о масштабировании факторов. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WfHLiGKkBLu"
   },
   "source": [
    "### Задание 4.5.\n",
    "Нормализуйте предикторы в обучающей и валидационной выборках с помощью MinMaxScaler из библиотеки sklearn. Помните, что обучение нормализатора производится на обучающей выборке, а трансформация на обучающей и валидационной!\n",
    "\n",
    "Рассчитайте среднее арифметическое для первого предиктора (т. е. для первого столбца матрицы) из валидационной выборки. Ответ округлите до сотых.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSslyUgwkBLu"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "\n",
    "print('Mean value of the first predictor (validation sample): {:.2f}'.format(\n",
    "    np.mean(X_valid[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaEMuA6skBLu"
   },
   "source": [
    "## 5. Решение задачи регрессии: линейная регрессия и деревья решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-pS_2XykBLu"
   },
   "source": [
    "Определим метрику, по которой мы будем измерять качество наших моделей. Мы будем следовать канонам исходного соревнования на Kaggle и в качестве метрики использовать RMSLE (Root Mean Squared Log Error), которая вычисляется как:\n",
    "$$RMSLE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(log(y_i+1)-log(\\hat{y_i}+1))^2},$$\n",
    "где:\n",
    "* $y_i$ - истинная длительность i-ой поездки на такси (trip_duration)\n",
    "* $\\hat{y_i}$- предсказанная моделью длительность i-ой поездки на такси\n",
    "\n",
    "Заметим, что логарифмирование целевого признака мы уже провели заранее, поэтому нам будет достаточно вычислить метрику RMSE для модели, обученной прогнозировать длительность поездки такси в логарифмическом масштабе:\n",
    "$$z_i=log(y_i+1),$$\n",
    "$$RMSLE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(z_i-\\hat{z_i})^2}=\\sqrt{MSE(z_i,\\hat{z_i})}$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Qd3xY1-kBLu"
   },
   "source": [
    "### Задание 5.1.\n",
    "Постройте модель линейной регрессии на обучающей выборке (факторы должны быть нормализованы, целевую переменную используйте в логарифмическом масштабе). Все параметры оставьте по умолчанию.\n",
    "\n",
    "Для полученной модели рассчитайте метрику RMSLE на тренировочной и валидационной выборках. Ответ округлите до сотых.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1IB1yK5kBLu"
   },
   "outputs": [],
   "source": [
    "lin_regr = linear_model.LinearRegression()\n",
    "lin_regr.fit(X_train, y_train_log)\n",
    "y_pred_train_log = lin_regr.predict(X_train)\n",
    "y_pred_valid_log = lin_regr.predict(X_valid)\n",
    "\n",
    "print('RMSLE on the train sample: {:.2f}'.format(\n",
    "    np.sqrt(metrics.mean_squared_error(y_train_log, y_pred_train_log))))\n",
    "print('RMSLE on the validation sample: {:.2f}'.format(\n",
    "    np.sqrt(metrics.mean_squared_error(y_valid_log, y_pred_valid_log))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSN6i928kBLu"
   },
   "source": [
    "### Задание 5.2.\n",
    "Сгенерируйте полиномиальные признаки 2-ой степени с помощью PolynomialFeatures из библиотеки sklearn. Параметр include_bias выставите в значение False.\n",
    "\n",
    "Постройте модель полиномиальной регрессии 2-ой степени на обучающей выборке (факторы должны быть нормализованы, целевую переменную используйте в логарифмическом масштабе). Все параметры оставьте по умолчанию.\n",
    "\n",
    "а) Для полученной модели рассчитайте метрику RMSLE на тренировочной и валидационной выборках. Ответ округлите до сотых.\n",
    "\n",
    "б) Наблюдаются ли у вашей модели признаки переобучения?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIIWA_arkBLu"
   },
   "outputs": [],
   "source": [
    "poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_valid_poly = poly.transform(X_valid)\n",
    "\n",
    "lin_regr_poly = linear_model.LinearRegression()\n",
    "lin_regr_poly.fit(X_train_poly, y_train_log)\n",
    "y_pred_train_log = lin_regr_poly.predict(X_train_poly)\n",
    "y_pred_valid_log = lin_regr_poly.predict(X_valid_poly)\n",
    "\n",
    "print('RMSLE on the train sample: {:.2f}'.format(\n",
    "    np.sqrt(metrics.mean_squared_error(y_train_log, y_pred_train_log))))\n",
    "print('RMSLE on the validation sample: {:.2f}'.format(\n",
    "    np.sqrt(metrics.mean_squared_error(y_valid_log, y_pred_valid_log))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ChD_MF6kBLv"
   },
   "source": [
    "### Задание 5.3.\n",
    "Постройте модель полиномиальной регрессии 2-ой степени с L2-регуляризацией (регуляризация по Тихонову) на обучающей выборке  (факторы должны быть нормализованы, целевую переменную используйте в логарифмическом масштабе). Коэффициент регуляризации $\\alpha$ установите равным 1, остальные параметры оставьте по умолчанию.\n",
    "\n",
    "Для полученной модели рассчитайте метрику RMSLE на тренировочной и валидационной выборках. Ответ округлите до сотых.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwcMCt3XkBLv"
   },
   "outputs": [],
   "source": [
    "lin_regr_ridge = linear_model.Ridge(alpha=1)\n",
    "lin_regr_ridge.fit(X_train_poly, y_train_log)\n",
    "y_pred_train_log = lin_regr_ridge.predict(X_train_poly)\n",
    "y_pred_valid_log = lin_regr_ridge.predict(X_valid_poly)\n",
    "\n",
    "print('RMSLE on the train sample: {:.2f}'.format(\n",
    "    np.sqrt(metrics.mean_squared_error(y_train_log, y_pred_train_log))))\n",
    "print('RMSLE on the validation sample: {:.2f}'.format(\n",
    "    np.sqrt(metrics.mean_squared_error(y_valid_log, y_pred_valid_log))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU5EJsIqkBLv"
   },
   "source": [
    "### Задание 5.4.\n",
    "Постройте модель дерева решений (DecisionTreeRegressor) на обучающей выборке (факторы должны быть нормализованы, целевую переменную используйте в логарифмическом масштабе). Все параметры оставьте по умолчанию. \n",
    "\n",
    "а) Для полученной модели рассчитайте метрику RMSLE на тренировочной и валидационной выборках. Ответ округлите до сотых.\n",
    "\n",
    "б) Наблюдаются ли у вашей модели признаки переобучения?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY1G4YcrkBLv"
   },
   "outputs": [],
   "source": [
    "dec_tree = tree.DecisionTreeRegressor()\n",
    "dec_tree.fit(X_train, y_train_log)\n",
    "y_pred_train_log = dec_tree.predict(X_train)\n",
    "y_pred_valid_log = dec_tree.predict(X_valid)\n",
    "\n",
    "print('RMSLE on the train sample: {:.2f}'.format(\n",
    "    np.sqrt(metrics.mean_squared_error(y_train_log, y_pred_train_log))))\n",
    "print('RMSLE on the validation sample: {:.2f}'.format(\n",
    "    np.sqrt(metrics.mean_squared_error(y_valid_log, y_pred_valid_log))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YJIlj4ykBLv"
   },
   "source": [
    "### Задание 5.5.\n",
    "Переберите все возможные варианты глубины дерева решений в диапазоне от 7 до 20:\n",
    "\n",
    "max_depths = range(7, 20)\n",
    "\n",
    "Параметр random_state задайте равным 42.\n",
    "\n",
    "Постройте линейные графики изменения метрики RMSE на тренировочной и валидационной выборках в зависимости от значения параметра глубины дерева решений. \n",
    "\n",
    "а) Найдите оптимальное значение максимальной глубины дерева, для которой будет наблюдаться минимальное значение RMSLE на обучающей выборке, но при этом еще не будет наблюдаться переобучение (валидационная кривая еще не начинает возрастать).\n",
    "\n",
    "б) Чему равно значение метрик RMSLE на тренировочной и валидационной выборках для дерева решений с выбранной оптимальной глубиной? Ответ округлите до сотых.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MbTVYsWkBLv"
   },
   "outputs": [],
   "source": [
    "max_depths = range(7, 20)\n",
    "train_scores, valid_scores = [], []\n",
    "for max_depth in max_depths:\n",
    "    dec_tree = tree.DecisionTreeRegressor(max_depth=max_depth)\n",
    "    dec_tree.fit(X_train, y_train_log)\n",
    "    y_pred_train_log = dec_tree.predict(X_train)\n",
    "    y_pred_valid_log = dec_tree.predict(X_valid)\n",
    "    train_scores.append(\n",
    "        np.sqrt(metrics.mean_squared_error(y_train_log, y_pred_train_log)))\n",
    "    valid_scores.append(\n",
    "        np.sqrt(metrics.mean_squared_error(y_valid_log, y_pred_valid_log)))\n",
    "    print('Calculations for max_depth = {} have been finished'.format(\n",
    "        max_depth))\n",
    "    \n",
    "fig, axes = plt.subplots(figsize=(8, 5))\n",
    "axes.plot(max_depths, train_scores, label='Train sample RMSLE')\n",
    "axes.plot(max_depths, valid_scores, label='Validation sample RMSLE')\n",
    "axes.set_title('Validation curve')\n",
    "axes.set_xlabel('Maximum depth of the tree')\n",
    "axes.set_ylabel('Metrics')\n",
    "axes.legend()\n",
    "\n",
    "index_best = np.argmin(valid_scores)\n",
    "print('Best tree depth: {}'.format(index_best + 7))\n",
    "print('Train sample RMSLE at the best depth: {:.2f}'.format(\n",
    "    train_scores[index_best]))\n",
    "print('Validation sample RMSLE at the best depth: {:.2f}'.format(\n",
    "    valid_scores[index_best]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUXrz7YDkBLv"
   },
   "source": [
    "## 6. Решение задачи регрессии: ансамблевые методы и построение прогноза"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "297rnbaikBLv"
   },
   "source": [
    "Переходим к тяжелой артиллерии: ансамблевым алгоритмам. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIAGlVjqkBLw"
   },
   "source": [
    "### Задание 6.1.\n",
    "\n",
    "Постройте модель случайного леса на обучающей выборке (факторы должны быть нормализованы, целевую переменную используйте в логарифмическом масштабе). В качестве гиперпараметров укажите следующие:\n",
    "* n_estimators=200,\n",
    "* max_depth=12,\n",
    "* criterion='squared_error',\n",
    "* min_samples_split=20,\n",
    "* random_state=42\n",
    "\n",
    "Для полученной модели рассчитайте метрику RMSLE на тренировочной и валидационной выборках. Ответ округлите до сотых.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQQdFHWfkBLw"
   },
   "outputs": [],
   "source": [
    "rforest = ensemble.RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=12, criterion='squared_error', \n",
    "    min_samples_split=20, random_state=42, n_jobs=-1)\n",
    "rforest.fit(X_train, y_train_log)\n",
    "y_pred_train_log = rforest.predict(X_train)\n",
    "y_pred_valid_log = rforest.predict(X_valid)\n",
    "\n",
    "print('RMSLE on the train sample: {:.2f}'.format(\n",
    "    np.sqrt(metrics.mean_squared_error(y_train_log, y_pred_train_log))))\n",
    "print('RMSLE on the validation sample: {:.2f}'.format(\n",
    "    np.sqrt(metrics.mean_squared_error(y_valid_log, y_pred_valid_log))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZz_qWKDkBLw"
   },
   "source": [
    "### Задание 6.2.\n",
    "Постройте модель градиентного бустинга над деревьями решений (GradientBoostingRegressor) на обучающей выборке (факторы должны быть нормализованы, целевую переменную используйте в логарифмическом масштабе). В качестве гиперпараметров укажите следующие:\n",
    "* learning_rate=0.5,\n",
    "* n_estimators=100,\n",
    "* max_depth=6, \n",
    "* min_samples_split=30,\n",
    "* random_state=42\n",
    "\n",
    "Для полученной модели рассчитайте метрику RMSLE на тренировочной и валидационной выборках. Ответ округлите до сотых.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnnmuhoSkBLw"
   },
   "outputs": [],
   "source": [
    "gboosting = ensemble.GradientBoostingRegressor(\n",
    "    learning_rate=0.5, n_estimators=100, max_depth=6, \n",
    "    min_samples_split=30, random_state=42, verbose=True)\n",
    "gboosting.fit(X_train, y_train_log)\n",
    "y_pred_train_log = gboosting.predict(X_train)\n",
    "y_pred_valid_log = gboosting.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC3NKXLhkBLw"
   },
   "source": [
    "### Задание 6.3.\n",
    "Какая из построенных вами моделей показала наилучший результат (наименьшее значение RMSLE на валидационной выборке)?\n",
    "* Линейная регрессия\n",
    "* Полиномиальная регрессия 2ой степени\n",
    "* Дерево решений\n",
    "* Случайный лес\n",
    "* Градиентный бустинг над деревьями решений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaBYTxnkkBLx"
   },
   "source": [
    "### Задание 6.4.\n",
    "Постройте столбчатую диаграмму коэффициентов значимости каждого из факторов.\n",
    "\n",
    "Укажите топ-3 наиболее значимых для предсказания целевого признака - длительности поездки в логарифмическом масштабе - факторов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmYOePR8kBLx"
   },
   "outputs": [],
   "source": [
    "features = pd.Series(\n",
    "    data=gboosting.feature_importances_,\n",
    "    index=selected_features)\n",
    "features.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 5))\n",
    "plt.bar(features.index,  features.values)\n",
    "axes.xaxis.set_tick_params(rotation=90)\n",
    "axes.set_title('Feature importances', fontsize=16)\n",
    "axes.set_ylabel('Feature importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQ4NzZaTkBLx"
   },
   "source": [
    "### Задание 6.5.\n",
    "Для лучшей из построенных моделей рассчитайте медианную абсолютную ошибку (MeAE - в sklearn функция median_absolute_error) предсказания длительности поездки такси на валидационной выборке:\n",
    "$$ MeAE = median(|y_i-\\hat{y_i}|)$$\n",
    "\n",
    "Значение метрики MeAE переведите в минуты и округлите до десятых.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOYteyJ-kBLx"
   },
   "outputs": [],
   "source": [
    "y_pred_valid = np.exp(y_pred_valid_log) - 1\n",
    "y_valid = np.exp(y_valid_log) - 1\n",
    "print('MeAE on the validation sample: {:.1f} min'.format(\n",
    "    metrics.median_absolute_error(y_valid, y_pred_valid) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZzsSUFfkBLx"
   },
   "source": [
    "Финальный шаг - сделать submit -  предсказание для отложенного тестового набора данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J12UJrZOkBLx"
   },
   "source": [
    "Прочитаем тестовые данные и заранее выделим столбец с идентификаторами поездок из тестового набора данных. Он нам еще пригодится:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KAzvZidkBLx"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "osrm_data_test = pd.read_csv(\"data/osrm_data_test.csv\")\n",
    "test_id = test_data['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdrR42JrkBLy"
   },
   "source": [
    "Перед созданием прогноза для тестовой выборки необходимо произвести все манипуляции с данными, которые мы производили с тренировочной выборкой, а именно:\n",
    "* Перевести признак pickup_datetime в формат datetime;\n",
    "* Добавить новые признаки (временные, географические, погодные и другие факторы);\n",
    "* Произвести очистку данных от пропусков;\n",
    "* Произвести кодировку категориальных признаков:\n",
    "    * Закодировать бинарные признаки;\n",
    "    * Закодировать номинальные признаки с помощью обученного на тренировочной выборке OneHotEncoder’а;\n",
    "* Сформировать матрицу наблюдений, оставив в таблице только те признаки, которые были отобраны с помощью SelectKBest;\n",
    "* Нормализовать данные с помощью обученного на тренировочной выборке MinMaxScaler’а.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqWQbqhGkBLy"
   },
   "outputs": [],
   "source": [
    "test_data['pickup_datetime']=pd.to_datetime(test_data['pickup_datetime'],format='%Y-%m-%d %H:%M:%S')\n",
    "test_data = add_datetime_features(test_data)\n",
    "test_data = add_holiday_features(test_data, holiday_data)\n",
    "test_data = add_osrm_features(test_data, osrm_data_test)\n",
    "test_data = add_geographical_features(test_data)\n",
    "test_data = add_cluster_features(test_data, kmeans)\n",
    "test_data = add_weather_features(test_data, weather_data)\n",
    "test_data = fill_null_weather_data(test_data)\n",
    "\n",
    "test_data['vendor_id'] = test_data['vendor_id'].apply(lambda x: 0 if x == 1 else 1)\n",
    "test_data['store_and_fwd_flag'] = test_data['store_and_fwd_flag'].apply(lambda x: 0 if x == 'N' else 1)\n",
    "test_data_onehot = one_hot_encoder.fit_transform(test_data[columns_to_change]).toarray()\n",
    "column_names = one_hot_encoder.get_feature_names_out(columns_to_change)\n",
    "test_data_onehot = pd.DataFrame(test_data_onehot, columns=column_names)\n",
    "\n",
    "test_data = pd.concat(\n",
    "    [test_data.reset_index(drop=True).drop(columns_to_change, axis=1), test_data_onehot], \n",
    "    axis=1\n",
    ")\n",
    "X_test = test_data[best_features]\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print('Shape of data: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24JJ64T4kBLy"
   },
   "source": [
    "Только после выполнения всех этих шагов можно сделать предсказание длительности поездки для тестовой выборки. Не забудьте перевести предсказания из логарифмического масштаба в истинный, используя формулу:\n",
    "$$y_i=exp(z_i)-1$$\n",
    "\n",
    "После того, как вы сформируете предсказание длительности поездок на тестовой выборке вам необходимо будет создать submission-файл в формате csv, отправить его на платформу Kaggle и посмотреть на результирующее значение метрики RMSLE на тестовой выборке.\n",
    "\n",
    "Код для создания submission-файла:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeMAzrHHkBLy"
   },
   "outputs": [],
   "source": [
    "y_pred_test = np.exp(gboosting.predict(X_test_scaled)) - 1\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'trip_duration': y_test_predict})\n",
    "submission.to_csv('data/submission_gb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU5BA_XDkBLy"
   },
   "source": [
    "### **В качестве бонуса**\n",
    "\n",
    "В завершение по ансамблевым мы предлагаем вам попробовать улучшить свое предсказание, воспользовавшись моделью экстремального градиентного бустинга (XGBoost) из библиотеки xgboost.\n",
    "\n",
    "**XGBoost** - современная модель машинного обучения, которая является продолжением идеи градиентного бустинга Фридмана. У нее есть несколько преимуществ по сравнению с классической моделью градиентного бустинга из библиотеки sklearn: повышенная производительность путем параллелизации процесса обучения, повышенное качество решения за счет усовершенствования алгоритма бустинга, меньшая склонность к переобучению и широкий функционал возможности управления параметрами модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FARWLbakBLy"
   },
   "source": [
    "Для ее использования необходимо для начала установить пакет xgboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8KXUhyGkBLy"
   },
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urka0597kBLz"
   },
   "source": [
    "После чего модуль можно импортировать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpLIalxfkBLz"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwJ80NU7kBLz"
   },
   "source": [
    "Перед обучением модели необходимо перевести наборы данных в тип данных xgboost.DMatrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehrdmZJekBLz"
   },
   "outputs": [],
   "source": [
    "# Создание матриц наблюдений в формате DMatrix\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train_log, feature_names=best_features)\n",
    "dvalid = xgb.DMatrix(X_valid_scaled, label=y_valid_log, feature_names=best_features)\n",
    "dtest = xgb.DMatrix(X_test_scaled, feature_names=best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW0LAucxkBLz"
   },
   "source": [
    "Обучение модели XGBoost происходит с помощью метода train, в который необходимо передать параметры модели, набор данных, количество базовых моделей в ансамбле, а также дополнительные параметры:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0uDs1YakBLz"
   },
   "outputs": [],
   "source": [
    "# Гиперпараметры модели\n",
    "xgb_pars = {'min_child_weight': 20, 'eta': 0.1, 'colsample_bytree': 0.9, \n",
    "            'max_depth': 6, 'subsample': 0.9, 'lambda': 1, 'nthread': -1, \n",
    "            'booster' : 'gbtree', 'eval_metric': 'rmse', 'objective': 'reg:squarederror'\n",
    "           }\n",
    "# Тренировочная и валидационная выборка\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "# Обучаем модель XGBoost\n",
    "model = xgb.train(\n",
    "    params=xgb_pars, #гиперпараметры модели\n",
    "    dtrain=dtrain, #обучающая выборка\n",
    "    num_boost_round=300, #количество моделей в ансамбле\n",
    "    evals=watchlist, #выборки, на которых считается матрица\n",
    "    early_stopping_rounds=20, #раняя остановка\n",
    "    maximize=False, #смена поиска максимума на минимум\n",
    "    verbose_eval=10 #шаг, через который происходит отображение метрик\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDiMljt5kBLz"
   },
   "source": [
    "Предсказать целевой признак на новых данных можно с помощью метода predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYBTeO7UkBLz"
   },
   "outputs": [],
   "source": [
    "#Делаем предсказание на тестовом наборе данных\n",
    "y_test_predict = np.exp(model.predict(dtest)) - 1\n",
    "print('Modeling RMSLE %.5f' % model.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgBn4xmikBLz"
   },
   "source": [
    "Также как и все модели, основанные на использовании деревьев решений в качестве базовых моделей, XGBoost имеет возможность определения коэффициентов важности факторов. Более того, в библиотеку встроена возможность визуализации важность факторов в виде столбчатой диаграммы. За эту возможность отвечает функция plot_importance():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEvlaqz4kBLz"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,15))\n",
    "xgb.plot_importance(model, ax = ax, height=0.5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
